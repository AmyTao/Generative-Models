{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_model_summary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**\n",
    "\n",
    "The presented code is not optimized, it serves an educational purpose. It is written for CPU, it uses only fully-connected networks and an extremely simplistic dataset. However, it contains all components that can help to understand how an autoregressive model (ARM) works, and it should be rather easy to extend it to more sophisticated models. This code could be run almost on any laptop/PC, and it takes a couple of minutes top to get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we go wild and use a dataset that is simpler than MNIST! We use a scipy dataset called Digits. It consists of ~1500 images of size 8x8, and each pixel can take values in $\\{0, 1, \\ldots, 16\\}$.\n",
    "\n",
    "The goal of using this dataset is that everyone can run it on a laptop, without any gpu etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "        elif mode == 'val':\n",
    "            self.data = digits.data[1000:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1350:].astype(np.float32)\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARM code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the blogpost for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    A causal 1D convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, A=False, **kwargs):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "\n",
    "        # attributes:\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.A = A\n",
    "        \n",
    "        self.padding = (kernel_size - 1) * dilation + A * 1\n",
    "\n",
    "        # module:\n",
    "        self.conv1d = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                      kernel_size, stride=1,\n",
    "                                      padding=0,\n",
    "                                      dilation=dilation,\n",
    "                                      **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.pad(x, (self.padding, 0))\n",
    "        conv1d_out = self.conv1d(x)\n",
    "        if self.A:\n",
    "            return conv1d_out[:, :, : -1]\n",
    "        else:\n",
    "            return conv1d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1.e-5\n",
    "\n",
    "def log_categorical(x, p, num_classes=256, reduction=None, dim=None):\n",
    "    #print(\"here\")\n",
    "    # print(x.long())\n",
    "    #print(x.shape)\n",
    "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
    "    #print(x_one_hot)\n",
    "    #print(x_one_hot.shape)\n",
    "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Q2\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARM(nn.Module):\n",
    "    #def __init__(self, net, D=2, num_vals=256):\n",
    "    def __init__(self, net, k,D=2 ,num_vals=256):\n",
    "        super(ARM, self).__init__()\n",
    "\n",
    "        print('ARM by JT.')\n",
    "\n",
    "        self.net = net\n",
    "        # TODO: Q2\n",
    "        # self.t = torch.nn.ModuleList([net() for _ in range(k)])\n",
    "        # self.s = torch.nn.ModuleList([net() for _ in range(k)])\n",
    "        # self.pi = torch.nn.ModuleList([net() for _ in range(k)])\n",
    "\n",
    "        self.num_vals = num_vals\n",
    "        self.D = D\n",
    "\n",
    "    def f(self, x):\n",
    "#         print(\"x\")\n",
    "#         print(x.unsqueeze(1).shape)\n",
    "        h = self.net(x.unsqueeze(1))\n",
    "\n",
    "        h = h.permute(0, 2, 1) #why permute here\n",
    "\n",
    "        p = torch.softmax(h, 2)\n",
    "\n",
    "        return p\n",
    "        \n",
    "    def forward(self, x, reduction='avg'):\n",
    "        if reduction == 'avg':\n",
    "            return -(self.log_prob(x).mean())\n",
    "        elif reduction == 'sum':\n",
    "            return -(self.log_prob(x).sum())\n",
    "        else:\n",
    "            raise ValueError('reduction could be either `avg` or `sum`.')\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        # print(\"x\")\n",
    "        # print(x.shape)\n",
    "        mu_d = self.f(x)\n",
    "        log_p = log_categorical(x, mu_d, num_classes=self.num_vals, reduction='sum', dim=-1).sum(-1)\n",
    "        \n",
    "        return log_p\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        x_new = torch.zeros((batch_size, self.D))\n",
    "\n",
    "        for d in range(self.D):\n",
    "            p = self.f(x_new)\n",
    "            x_new_d = torch.multinomial(p[:, d, :], num_samples=1)\n",
    "            x_new[:, d] = x_new_d[:,0]\n",
    "\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions: training, evaluation, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather self-explanatory, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, test_batch in enumerate(test_loader):\n",
    "        loss_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def samples_real(name, test_loader):\n",
    "    # REAL-------\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = next(iter(test_loader)).detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def samples_generated(name, data_loader, extra_name=''):\n",
    "    x = next(iter(data_loader)).detach().numpy()\n",
    "\n",
    "    # GENERATIONS-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = model_best.sample(num_x * num_y)\n",
    "    x = x.detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_curve(name, nll_val):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('nll')\n",
    "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            if hasattr(model, 'dequantization'):\n",
    "                if model.dequantization:\n",
    "                    batch = batch + torch.rand(batch.shape)\n",
    "            loss = model.forward(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "        else:\n",
    "            if loss_val < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_val\n",
    "                patience = 0\n",
    "\n",
    "                samples_generated(name, val_loader, extra_name=\"_epoch_\" + str(e))\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Digits(mode='train')\n",
    "val_data = Digits(mode='val')\n",
    "test_data = Digits(mode='test')\n",
    "\n",
    "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "result_dir = 'results/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "name = 'arm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64   # input dimension\n",
    "M = 256  # the number of neurons in scale (s) and translation (t) nets\n",
    "\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 100 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARM by JT.\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "    CausalConv1d-1        [1, 256, 64]           2,048           2,048\n",
      "       LeakyReLU-2        [1, 256, 64]               0               0\n",
      "    CausalConv1d-3        [1, 256, 64]         459,008         459,008\n",
      "       LeakyReLU-4        [1, 256, 64]               0               0\n",
      "    CausalConv1d-5        [1, 256, 64]         459,008         459,008\n",
      "       LeakyReLU-6        [1, 256, 64]               0               0\n",
      "    CausalConv1d-7         [1, 17, 64]          30,481          30,481\n",
      "=======================================================================\n",
      "Total params: 950,545\n",
      "Trainable params: 950,545\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "likelihood_type = 'categorical'\n",
    "\n",
    "num_vals = 17\n",
    "\n",
    "kernel = 7\n",
    "\n",
    "net = nn.Sequential(\n",
    "    CausalConv1d(in_channels=1, out_channels=M, dilation=1, kernel_size=kernel, A=True, bias=True),\n",
    "    nn.LeakyReLU(),\n",
    "    CausalConv1d(in_channels=M, out_channels=M, dilation=1, kernel_size=kernel, A=False, bias=True),\n",
    "    nn.LeakyReLU(),\n",
    "    CausalConv1d(in_channels=M, out_channels=M, dilation=1, kernel_size=kernel, A=False, bias=True),\n",
    "    nn.LeakyReLU(),\n",
    "    CausalConv1d(in_channels=M, out_channels=num_vals, dilation=1, kernel_size=kernel, A=False, bias=True))\n",
    "\n",
    "nets = nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2), nn.Tanh())\n",
    "\n",
    "\n",
    "#model = ARM(net, D=D, num_vals=num_vals)\n",
    "model = ARM(net, nets, D=D, num_vals=num_vals)\n",
    "\n",
    "# Print the summary (like in Keras)\n",
    "print(summary(model, torch.zeros(1, 64), show_input=False, show_hierarchical=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play! Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val nll=119.13344587053571\n",
      "saved!\n",
      "Epoch: 1, val nll=113.38597377232144\n",
      "saved!\n",
      "Epoch: 2, val nll=110.37569754464286\n",
      "saved!\n",
      "Epoch: 3, val nll=108.64208844866072\n",
      "saved!\n",
      "Epoch: 4, val nll=107.06242885044642\n",
      "saved!\n",
      "Epoch: 5, val nll=105.69672572544643\n",
      "saved!\n",
      "Epoch: 6, val nll=103.63041782924107\n",
      "saved!\n",
      "Epoch: 7, val nll=101.57997767857142\n",
      "saved!\n",
      "Epoch: 8, val nll=99.70226841517857\n",
      "saved!\n",
      "Epoch: 9, val nll=98.13289760044643\n",
      "saved!\n",
      "Epoch: 10, val nll=97.10820591517857\n",
      "saved!\n",
      "Epoch: 11, val nll=96.14623116629464\n",
      "saved!\n",
      "Epoch: 12, val nll=94.8891845703125\n",
      "saved!\n",
      "Epoch: 13, val nll=94.12801478794643\n",
      "saved!\n",
      "Epoch: 14, val nll=93.43857700892858\n",
      "saved!\n",
      "Epoch: 15, val nll=93.01925013950893\n",
      "saved!\n",
      "Epoch: 16, val nll=92.6134228515625\n",
      "saved!\n",
      "Epoch: 17, val nll=92.35888462611607\n",
      "saved!\n",
      "Epoch: 18, val nll=92.25493024553572\n",
      "saved!\n",
      "Epoch: 19, val nll=91.77373116629464\n",
      "saved!\n",
      "Epoch: 20, val nll=91.62992327008928\n",
      "saved!\n",
      "Epoch: 21, val nll=91.29877720424108\n",
      "saved!\n",
      "Epoch: 22, val nll=91.26697614397321\n",
      "saved!\n",
      "Epoch: 23, val nll=91.09595982142856\n",
      "saved!\n",
      "Epoch: 24, val nll=90.69563895089286\n",
      "saved!\n",
      "Epoch: 25, val nll=90.38662737165178\n",
      "saved!\n",
      "Epoch: 26, val nll=90.49961356026786\n",
      "Epoch: 27, val nll=90.26164341517857\n",
      "saved!\n",
      "Epoch: 28, val nll=90.26865025111607\n",
      "Epoch: 29, val nll=90.252998046875\n",
      "saved!\n",
      "Epoch: 30, val nll=89.91886300223214\n",
      "saved!\n",
      "Epoch: 31, val nll=90.30083775111608\n",
      "Epoch: 32, val nll=89.91763602120535\n",
      "saved!\n",
      "Epoch: 33, val nll=89.73344029017858\n",
      "saved!\n",
      "Epoch: 34, val nll=89.96938755580358\n",
      "Epoch: 35, val nll=89.85221609933036\n",
      "Epoch: 36, val nll=89.75809361049107\n",
      "Epoch: 37, val nll=89.52022739955358\n",
      "saved!\n",
      "Epoch: 38, val nll=89.68889787946429\n",
      "Epoch: 39, val nll=89.27448869977678\n",
      "saved!\n",
      "Epoch: 40, val nll=89.43028250558035\n",
      "Epoch: 41, val nll=89.46241420200893\n",
      "Epoch: 42, val nll=89.42980398995536\n",
      "Epoch: 43, val nll=89.69929408482143\n",
      "Epoch: 44, val nll=89.95136160714286\n",
      "Epoch: 45, val nll=89.36352260044643\n",
      "Epoch: 46, val nll=89.47669642857143\n",
      "Epoch: 47, val nll=89.725478515625\n",
      "Epoch: 48, val nll=89.55836007254464\n",
      "Epoch: 49, val nll=89.59521484375\n",
      "Epoch: 50, val nll=89.44606515066964\n",
      "Epoch: 51, val nll=89.85959612165179\n",
      "Epoch: 52, val nll=89.78061802455358\n",
      "Epoch: 53, val nll=89.56497279575893\n",
      "Epoch: 54, val nll=89.65250418526786\n",
      "Epoch: 55, val nll=89.77115443638392\n",
      "Epoch: 56, val nll=89.68295689174107\n",
      "Epoch: 57, val nll=89.96462193080357\n",
      "Epoch: 58, val nll=90.01529227120535\n",
      "Epoch: 59, val nll=90.07102957589285\n",
      "Epoch: 60, val nll=89.92890345982143\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "nll_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
    "                       training_loader=training_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL LOSS: nll=86.43572055893456\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "\n",
    "samples_real(result_dir + name, test_loader)\n",
    "\n",
    "plot_curve(result_dir + name, nll_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import weight_norm as wn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def concat_elu(x):\n",
    "    \"\"\" like concatenated ReLU (http://arxiv.org/abs/1603.05201), but then with ELU \"\"\"\n",
    "    # Pytorch ordering\n",
    "    axis = len(x.size()) - 3\n",
    "    return F.elu(torch.cat([x, -x], dim=axis))\n",
    "\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    \"\"\" numerically stable log_sum_exp implementation that prevents overflow \"\"\"\n",
    "    # TF ordering\n",
    "    axis  = len(x.size()) - 1\n",
    "    m, _  = torch.max(x, dim=axis)\n",
    "    m2, _ = torch.max(x, dim=axis, keepdim=True)\n",
    "    return m + torch.log(torch.sum(torch.exp(x - m2), dim=axis))\n",
    "\n",
    "\n",
    "def log_prob_from_logits(x):\n",
    "    \"\"\" numerically stable log_softmax implementation that prevents overflow \"\"\"\n",
    "    # TF ordering\n",
    "    axis = len(x.size()) - 1\n",
    "    m, _ = torch.max(x, dim=axis, keepdim=True)\n",
    "    return x - m - torch.log(torch.sum(torch.exp(x - m), dim=axis, keepdim=True))\n",
    "\n",
    "\n",
    "def discretized_mix_logistic_loss(x, l):\n",
    "    \"\"\" log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval \"\"\"\n",
    "    # Pytorch ordering\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    l = l.permute(0, 2, 3, 1)\n",
    "    xs = [int(y) for y in x.size()]\n",
    "    ls = [int(y) for y in l.size()]\n",
    "   \n",
    "    # here and below: unpacking the params of the mixture of logistics\n",
    "    nr_mix = int(ls[-1] / 10) \n",
    "    logit_probs = l[:, :, :, :nr_mix]\n",
    "    l = l[:, :, :, nr_mix:].contiguous().view(xs + [nr_mix * 3]) # 3 for mean, scale, coef\n",
    "    means = l[:, :, :, :, :nr_mix]\n",
    "    # log_scales = torch.max(l[:, :, :, :, nr_mix:2 * nr_mix], -7.)\n",
    "    log_scales = torch.clamp(l[:, :, :, :, nr_mix:2 * nr_mix], min=-7.)\n",
    "   \n",
    "    coeffs = F.tanh(l[:, :, :, :, 2 * nr_mix:3 * nr_mix])\n",
    "    # here and below: getting the means and adjusting them based on preceding\n",
    "    # sub-pixels\n",
    "    x = x.contiguous()\n",
    "    x = x.unsqueeze(-1) + Variable(torch.zeros(xs + [nr_mix]).cuda(), requires_grad=False)\n",
    "    m2 = (means[:, :, :, 1, :] + coeffs[:, :, :, 0, :]\n",
    "                * x[:, :, :, 0, :]).view(xs[0], xs[1], xs[2], 1, nr_mix)\n",
    "\n",
    "    m3 = (means[:, :, :, 2, :] + coeffs[:, :, :, 1, :] * x[:, :, :, 0, :] +\n",
    "                coeffs[:, :, :, 2, :] * x[:, :, :, 1, :]).view(xs[0], xs[1], xs[2], 1, nr_mix)\n",
    "\n",
    "    means = torch.cat((means[:, :, :, 0, :].unsqueeze(3), m2, m3), dim=3)\n",
    "    centered_x = x - means\n",
    "    inv_stdv = torch.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_x + 1. / 255.)\n",
    "    cdf_plus = F.sigmoid(plus_in)\n",
    "    min_in = inv_stdv * (centered_x - 1. / 255.)\n",
    "    cdf_min = F.sigmoid(min_in)\n",
    "    # log probability for edge case of 0 (before scaling)\n",
    "    log_cdf_plus = plus_in - F.softplus(plus_in)\n",
    "    # log probability for edge case of 255 (before scaling)\n",
    "    log_one_minus_cdf_min = -F.softplus(min_in)\n",
    "    cdf_delta = cdf_plus - cdf_min  # probability for all other cases\n",
    "    mid_in = inv_stdv * centered_x\n",
    "    # log probability in the center of the bin, to be used in extreme cases\n",
    "    # (not actually used in our code)\n",
    "    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n",
    "\n",
    "    # now select the right output: left edge case, right edge case, normal\n",
    "    # case, extremely low prob case (doesn't actually happen for us)\n",
    "\n",
    "    # this is what we are really doing, but using the robust version below for extreme cases in other applications and to avoid NaN issue with tf.select()\n",
    "    # log_probs = tf.select(x < -0.999, log_cdf_plus, tf.select(x > 0.999, log_one_minus_cdf_min, tf.log(cdf_delta)))\n",
    "\n",
    "    # robust version, that still works if probabilities are below 1e-5 (which never happens in our code)\n",
    "    # tensorflow backpropagates through tf.select() by multiplying with zero instead of selecting: this requires use to use some ugly tricks to avoid potential NaNs\n",
    "    # the 1e-12 in tf.maximum(cdf_delta, 1e-12) is never actually used as output, it's purely there to get around the tf.select() gradient issue\n",
    "    # if the probability on a sub-pixel is below 1e-5, we use an approximation\n",
    "    # based on the assumption that the log-density is constant in the bin of\n",
    "    # the observed sub-pixel value\n",
    "    \n",
    "    inner_inner_cond = (cdf_delta > 1e-5).float()\n",
    "    inner_inner_out  = inner_inner_cond * torch.log(torch.clamp(cdf_delta, min=1e-12)) + (1. - inner_inner_cond) * (log_pdf_mid - np.log(127.5))\n",
    "    inner_cond       = (x > 0.999).float()\n",
    "    inner_out        = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n",
    "    cond             = (x < -0.999).float()\n",
    "    log_probs        = cond * log_cdf_plus + (1. - cond) * inner_out\n",
    "    log_probs        = torch.sum(log_probs, dim=3) + log_prob_from_logits(logit_probs)\n",
    "    \n",
    "    return -torch.sum(log_sum_exp(log_probs))\n",
    "\n",
    "\n",
    "def discretized_mix_logistic_loss_1d(x, l):\n",
    "    \"\"\" log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval \"\"\"\n",
    "    # Pytorch ordering\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    l = l.permute(0, 2, 3, 1)\n",
    "    xs = [int(y) for y in x.size()]\n",
    "    ls = [int(y) for y in l.size()]\n",
    "\n",
    "    # here and below: unpacking the params of the mixture of logistics\n",
    "    nr_mix = int(ls[-1] / 3)\n",
    "    logit_probs = l[:, :, :, :nr_mix]\n",
    "    l = l[:, :, :, nr_mix:].contiguous().view(xs + [nr_mix * 2]) # 2 for mean, scale\n",
    "    means = l[:, :, :, :, :nr_mix]\n",
    "    log_scales = torch.clamp(l[:, :, :, :, nr_mix:2 * nr_mix], min=-7.)\n",
    "    # here and below: getting the means and adjusting them based on preceding\n",
    "    # sub-pixels\n",
    "    x = x.contiguous()\n",
    "    x = x.unsqueeze(-1) + Variable(torch.zeros(xs + [nr_mix]).cuda(), requires_grad=False)\n",
    "\n",
    "    # means = torch.cat((means[:, :, :, 0, :].unsqueeze(3), m2, m3), dim=3)\n",
    "    centered_x = x - means\n",
    "    inv_stdv = torch.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_x + 1. / 255.)\n",
    "    cdf_plus = F.sigmoid(plus_in)\n",
    "    min_in = inv_stdv * (centered_x - 1. / 255.)\n",
    "    cdf_min = F.sigmoid(min_in)\n",
    "    # log probability for edge case of 0 (before scaling)\n",
    "    log_cdf_plus = plus_in - F.softplus(plus_in)\n",
    "    # log probability for edge case of 255 (before scaling)\n",
    "    log_one_minus_cdf_min = -F.softplus(min_in)\n",
    "    cdf_delta = cdf_plus - cdf_min  # probability for all other cases\n",
    "    mid_in = inv_stdv * centered_x\n",
    "    # log probability in the center of the bin, to be used in extreme cases\n",
    "    # (not actually used in our code)\n",
    "    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n",
    "    \n",
    "    inner_inner_cond = (cdf_delta > 1e-5).float()\n",
    "    inner_inner_out  = inner_inner_cond * torch.log(torch.clamp(cdf_delta, min=1e-12)) + (1. - inner_inner_cond) * (log_pdf_mid - np.log(127.5))\n",
    "    inner_cond       = (x > 0.999).float()\n",
    "    inner_out        = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n",
    "    cond             = (x < -0.999).float()\n",
    "    log_probs        = cond * log_cdf_plus + (1. - cond) * inner_out\n",
    "    log_probs        = torch.sum(log_probs, dim=3) + log_prob_from_logits(logit_probs)\n",
    "    \n",
    "    return -torch.sum(log_sum_exp(log_probs))\n",
    "\n",
    "\n",
    "def to_one_hot(tensor, n, fill_with=1.):\n",
    "    # we perform one hot encore with respect to the last axis\n",
    "    one_hot = torch.FloatTensor(tensor.size() + (n,)).zero_()\n",
    "    if tensor.is_cuda : one_hot = one_hot.cuda()\n",
    "    one_hot.scatter_(len(tensor.size()), tensor.unsqueeze(-1), fill_with)\n",
    "    return Variable(one_hot)\n",
    "\n",
    "\n",
    "def sample_from_discretized_mix_logistic_1d(l, nr_mix):\n",
    "    # Pytorch ordering\n",
    "    l = l.permute(0, 2, 3, 1)\n",
    "    ls = [int(y) for y in l.size()]\n",
    "    xs = ls[:-1] + [1] #[3]\n",
    "\n",
    "    # unpack parameters\n",
    "    logit_probs = l[:, :, :, :nr_mix]\n",
    "    l = l[:, :, :, nr_mix:].contiguous().view(xs + [nr_mix * 2]) # for mean, scale\n",
    "\n",
    "    # sample mixture indicator from softmax\n",
    "    temp = torch.FloatTensor(logit_probs.size())\n",
    "    if l.is_cuda : temp = temp.cuda()\n",
    "    temp.uniform_(1e-5, 1. - 1e-5)\n",
    "    temp = logit_probs.data - torch.log(- torch.log(temp))\n",
    "    _, argmax = temp.max(dim=3)\n",
    "   \n",
    "    one_hot = to_one_hot(argmax, nr_mix)\n",
    "    sel = one_hot.view(xs[:-1] + [1, nr_mix])\n",
    "    # select logistic parameters\n",
    "    means = torch.sum(l[:, :, :, :, :nr_mix] * sel, dim=4) \n",
    "    log_scales = torch.clamp(torch.sum(\n",
    "        l[:, :, :, :, nr_mix:2 * nr_mix] * sel, dim=4), min=-7.)\n",
    "    u = torch.FloatTensor(means.size())\n",
    "    if l.is_cuda : u = u.cuda()\n",
    "    u.uniform_(1e-5, 1. - 1e-5)\n",
    "    u = Variable(u)\n",
    "    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1. - u))\n",
    "    x0 = torch.clamp(torch.clamp(x[:, :, :, 0], min=-1.), max=1.)\n",
    "    out = x0.unsqueeze(1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def sample_from_discretized_mix_logistic(l, nr_mix):\n",
    "    # Pytorch ordering\n",
    "    l = l.permute(0, 2, 3, 1)\n",
    "    ls = [int(y) for y in l.size()]\n",
    "    xs = ls[:-1] + [3]\n",
    "\n",
    "    # unpack parameters\n",
    "    logit_probs = l[:, :, :, :nr_mix]\n",
    "    l = l[:, :, :, nr_mix:].contiguous().view(xs + [nr_mix * 3])\n",
    "    # sample mixture indicator from softmax\n",
    "    temp = torch.FloatTensor(logit_probs.size())\n",
    "    if l.is_cuda : temp = temp.cuda()\n",
    "    temp.uniform_(1e-5, 1. - 1e-5)\n",
    "    temp = logit_probs.data - torch.log(- torch.log(temp))\n",
    "    _, argmax = temp.max(dim=3)\n",
    "   \n",
    "    one_hot = to_one_hot(argmax, nr_mix)\n",
    "    sel = one_hot.view(xs[:-1] + [1, nr_mix])\n",
    "    # select logistic parameters\n",
    "    means = torch.sum(l[:, :, :, :, :nr_mix] * sel, dim=4) \n",
    "    log_scales = torch.clamp(torch.sum(\n",
    "        l[:, :, :, :, nr_mix:2 * nr_mix] * sel, dim=4), min=-7.)\n",
    "    coeffs = torch.sum(F.tanh(\n",
    "        l[:, :, :, :, 2 * nr_mix:3 * nr_mix]) * sel, dim=4)\n",
    "    # sample from logistic & clip to interval\n",
    "    # we don't actually round to the nearest 8bit value when sampling\n",
    "    u = torch.FloatTensor(means.size())\n",
    "    if l.is_cuda : u = u.cuda()\n",
    "    u.uniform_(1e-5, 1. - 1e-5)\n",
    "    u = Variable(u)\n",
    "    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1. - u))\n",
    "    x0 = torch.clamp(torch.clamp(x[:, :, :, 0], min=-1.), max=1.)\n",
    "    x1 = torch.clamp(torch.clamp(\n",
    "       x[:, :, :, 1] + coeffs[:, :, :, 0] * x0, min=-1.), max=1.)\n",
    "    x2 = torch.clamp(torch.clamp(\n",
    "       x[:, :, :, 2] + coeffs[:, :, :, 1] * x0 + coeffs[:, :, :, 2] * x1, min=-1.), max=1.)\n",
    "\n",
    "    out = torch.cat([x0.view(xs[:-1] + [1]), x1.view(xs[:-1] + [1]), x2.view(xs[:-1] + [1])], dim=3)\n",
    "    # put back in Pytorch ordering\n",
    "    out = out.permute(0, 3, 1, 2)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "''' utilities for shifting the image around, efficient alternative to masking convolutions '''\n",
    "def down_shift(x, pad=None):\n",
    "    # Pytorch ordering\n",
    "    xs = [int(y) for y in x.size()]\n",
    "    # when downshifting, the last row is removed \n",
    "    x = x[:, :, :xs[2] - 1, :]\n",
    "    # padding left, padding right, padding top, padding bottom\n",
    "    pad = nn.ZeroPad2d((0, 0, 1, 0)) if pad is None else pad\n",
    "    return pad(x)\n",
    "\n",
    "\n",
    "def right_shift(x, pad=None):\n",
    "    # Pytorch ordering\n",
    "    xs = [int(y) for y in x.size()]\n",
    "    # when righshifting, the last column is removed \n",
    "    x = x[:, :, :, :xs[3] - 1]\n",
    "    # padding left, padding right, padding top, padding bottom\n",
    "    pad = nn.ZeroPad2d((1, 0, 0, 0)) if pad is None else pad\n",
    "    return pad(x)\n",
    "\n",
    "\n",
    "def load_part_of_model(model, path):\n",
    "    params = torch.load(path)\n",
    "    added = 0\n",
    "    for name, param in params.items():\n",
    "        if name in model.state_dict().keys():\n",
    "            try : \n",
    "                model.state_dict()[name].copy_(param)\n",
    "                added += 1\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                pass\n",
    "    print('added %s of params:' % (added / float(len(model.state_dict().keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compared with RealNVP, the arm model converges much faster and the nll result is also smaller. The learning curve is much \n",
    "smoother than RealNVP. \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
