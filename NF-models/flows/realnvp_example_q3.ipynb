{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCollaborator: \\nZhijun Liu, who helps me a lot understanding the orthogonal initilization and kindly help me examined the gradient when\\nmy nll exploded :)\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Collaborator: \n",
    "Zhijun Liu, who helps me a lot understanding the orthogonal initilization and kindly help me examined the gradient when\n",
    "my nll exploded :)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_model_summary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**\n",
    "\n",
    "The presented code is not optimized, it serves an educational purpose. It is written for CPU, it uses only fully-connected networks and an extremely simplistic dataset. However, it contains all components that can help to understand how RealNVP works, and it should be rather easy to extend it to more sophisticated models. This code could be run almost on any laptop/PC, and it takes a couple of minutes top to get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we go wild and use a dataset that is simpler than MNIST! We use a scipy dataset called Digits. It consists of ~1500 images of size 8x8, and each pixel can take values in $\\{0, 1, \\ldots, 16\\}$.\n",
    "\n",
    "The goal of using this dataset is that everyone can run it on a laptop, without any gpu etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "        elif mode == 'val':\n",
    "            self.data = digits.data[1000:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1350:].astype(np.float32)\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RealNVP code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the blogpost for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_orthogonal = lambda shape: nn.init.orthogonal_((shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    \n",
    "    def __init__(self, nets, nett, num_flows, prior, D=2, dequantization=True):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        print('RealNVP by JT.')\n",
    "        \n",
    "        self.dequantization = dequantization\n",
    "        \n",
    "        self.prior = prior\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(num_flows)])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(num_flows)])\n",
    "        # TODO Q3: add orthogonal matrix\n",
    "        # orthogonal = lambda shape: nn.init.orthogonal_(torch.empty(shape))\n",
    "        # self.p = nn.ParameterList([nn.Parameter(orthogonal((D,D))) for _ in range(num_flows)])\n",
    "        self.p = torch.nn.ModuleList([nn.Linear(D, D, bias=False) for _ in range(num_flows)])\n",
    "        for layer in self.p:\n",
    "            torch.nn.init.orthogonal_(layer.weight)\n",
    "        \n",
    "        self.num_flows = num_flows\n",
    "        \n",
    "        self.D = D\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        # x: input, either images (for the first transformation) or outputs from the previous transformation\n",
    "        # index: it determines the index of the transformation\n",
    "        # forward: whether it is a pass from x to y (forward=True), or from y to x (forward=False)\n",
    "        \n",
    "     \n",
    "        (xa, xb) = torch.chunk(x, 2, 1) # split into 2 parts in dim=1\n",
    "\n",
    "\n",
    "        \n",
    "        s = self.s[index](xa)\n",
    "        t = self.t[index](xa)\n",
    "        \n",
    "        if forward:\n",
    "            #yb = f^{-1}(x)\n",
    "            yb = (xb - t) * torch.exp(-s)\n",
    "        else:\n",
    "            #xb = f(y)\n",
    "            yb = torch.exp(s) * xb + t\n",
    "        \n",
    "        return torch.cat((xa, yb), 1), s # recombine two parts along dim =1, return scale\n",
    "\n",
    "    def permute(self, x):\n",
    "        \n",
    "        return x.flip(1)\n",
    "    def permute(self,x,index,forward = True):\n",
    "        # TODO: Q3\n",
    "        if forward:\n",
    "            #print(x.shape)\n",
    "            #print(self.p[index].shape)\n",
    "            x = x @ self.p[index].weight\n",
    "            p = torch.det(self.p[index].weight).abs().log()\n",
    "        else:\n",
    "            x = x @ self.p[index].weight.inverse()\n",
    "            p = -torch.det(self.p[index].weight).abs().log()\n",
    "        return x,p\n",
    "\n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in range(self.num_flows):\n",
    "            z, s = self.coupling(z, i, forward=True)\n",
    "            z, p = self.permute(z,i,forward=True)\n",
    "            \n",
    "            log_det_J = log_det_J + p - s.sum(dim=1)\n",
    "             \n",
    "\n",
    "        return z, log_det_J\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            #x = self.permute(x)\n",
    "            x, _ = self.permute(x,i,forward =False)\n",
    "            x, _ = self.coupling(x, i, forward=False)\n",
    "            \n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        z, log_det_J = self.f(x)\n",
    "        if reduction == 'sum':\n",
    "            return -(self.prior.log_prob(z) + log_det_J).sum()\n",
    "        else:\n",
    "            return -(self.prior.log_prob(z) + log_det_J).mean()\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        z = self.prior.sample((batchSize, self.D)) #每一个维度的值有相关性，考虑prior是多元分布\n",
    "        z = z[:, 0, :] # 对z进行操作，抛弃了其中一个维度\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(-1, self.D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions: training, evaluation, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather self-explanatory, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, test_batch in enumerate(test_loader):\n",
    "        if hasattr(model, 'dequantization'):\n",
    "            if model.dequantization:\n",
    "                test_batch = test_batch + (1. - torch.rand(test_batch.shape))/2.\n",
    "        loss_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def samples_real(name, test_loader):\n",
    "    # REAL-------\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = next(iter(test_loader)).detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def samples_generated(name, data_loader, extra_name=''):\n",
    "    x = next(iter(data_loader)).detach().numpy()\n",
    "\n",
    "    # GENERATIONS-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = model_best.sample(num_x * num_y)\n",
    "    x = x.detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_curve(name, nll_val):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('nll')\n",
    "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            if hasattr(model, 'dequantization'):\n",
    "                if model.dequantization:\n",
    "                    batch = batch + (1. - torch.rand(batch.shape))/2.\n",
    "            loss = model.forward(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "        else:\n",
    "            if loss_val < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_val\n",
    "                patience = 0\n",
    "\n",
    "                samples_generated(name, val_loader, extra_name=\"_epoch_\" + str(e))\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Digits(mode='train')\n",
    "val_data = Digits(mode='val')\n",
    "test_data = Digits(mode='test')\n",
    "\n",
    "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "result_dir = 'results_q3/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "name = 'realnvp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64   # input dimension\n",
    "M = 256  # the number of neurons in scale (s) and translation (t) nets\n",
    "\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 1000 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RealNVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealNVP by JT.\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1            [1, 256]           8,448           8,448\n",
      "       LeakyReLU-2            [1, 256]               0               0\n",
      "          Linear-3            [1, 256]          65,792          65,792\n",
      "       LeakyReLU-4            [1, 256]               0               0\n",
      "          Linear-5             [1, 32]           8,224           8,224\n",
      "            Tanh-6             [1, 32]               0               0\n",
      "          Linear-7            [1, 256]           8,448           8,448\n",
      "       LeakyReLU-8            [1, 256]               0               0\n",
      "          Linear-9            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-10            [1, 256]               0               0\n",
      "         Linear-11             [1, 32]           8,224           8,224\n",
      "         Linear-12            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-13            [1, 256]               0               0\n",
      "         Linear-14            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-15            [1, 256]               0               0\n",
      "         Linear-16             [1, 32]           8,224           8,224\n",
      "           Tanh-17             [1, 32]               0               0\n",
      "         Linear-18            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-19            [1, 256]               0               0\n",
      "         Linear-20            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-21            [1, 256]               0               0\n",
      "         Linear-22             [1, 32]           8,224           8,224\n",
      "         Linear-23            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-24            [1, 256]               0               0\n",
      "         Linear-25            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-26            [1, 256]               0               0\n",
      "         Linear-27             [1, 32]           8,224           8,224\n",
      "           Tanh-28             [1, 32]               0               0\n",
      "         Linear-29            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-30            [1, 256]               0               0\n",
      "         Linear-31            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-32            [1, 256]               0               0\n",
      "         Linear-33             [1, 32]           8,224           8,224\n",
      "         Linear-34            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-35            [1, 256]               0               0\n",
      "         Linear-36            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-37            [1, 256]               0               0\n",
      "         Linear-38             [1, 32]           8,224           8,224\n",
      "           Tanh-39             [1, 32]               0               0\n",
      "         Linear-40            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-41            [1, 256]               0               0\n",
      "         Linear-42            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-43            [1, 256]               0               0\n",
      "         Linear-44             [1, 32]           8,224           8,224\n",
      "         Linear-45            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-46            [1, 256]               0               0\n",
      "         Linear-47            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-48            [1, 256]               0               0\n",
      "         Linear-49             [1, 32]           8,224           8,224\n",
      "           Tanh-50             [1, 32]               0               0\n",
      "         Linear-51            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-52            [1, 256]               0               0\n",
      "         Linear-53            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-54            [1, 256]               0               0\n",
      "         Linear-55             [1, 32]           8,224           8,224\n",
      "         Linear-56            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-57            [1, 256]               0               0\n",
      "         Linear-58            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-59            [1, 256]               0               0\n",
      "         Linear-60             [1, 32]           8,224           8,224\n",
      "           Tanh-61             [1, 32]               0               0\n",
      "         Linear-62            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-63            [1, 256]               0               0\n",
      "         Linear-64            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-65            [1, 256]               0               0\n",
      "         Linear-66             [1, 32]           8,224           8,224\n",
      "         Linear-67            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-68            [1, 256]               0               0\n",
      "         Linear-69            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-70            [1, 256]               0               0\n",
      "         Linear-71             [1, 32]           8,224           8,224\n",
      "           Tanh-72             [1, 32]               0               0\n",
      "         Linear-73            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-74            [1, 256]               0               0\n",
      "         Linear-75            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-76            [1, 256]               0               0\n",
      "         Linear-77             [1, 32]           8,224           8,224\n",
      "         Linear-78            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-79            [1, 256]               0               0\n",
      "         Linear-80            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-81            [1, 256]               0               0\n",
      "         Linear-82             [1, 32]           8,224           8,224\n",
      "           Tanh-83             [1, 32]               0               0\n",
      "         Linear-84            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-85            [1, 256]               0               0\n",
      "         Linear-86            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-87            [1, 256]               0               0\n",
      "         Linear-88             [1, 32]           8,224           8,224\n",
      "=======================================================================\n",
      "Total params: 1,319,424\n",
      "Trainable params: 1,319,424\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The number of invertible transformations\n",
    "num_flows = 8\n",
    "\n",
    "# scale (s) network i.e.variance\n",
    "nets = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2), nn.Tanh())\n",
    "\n",
    "# translation (t) network i.e. mean\n",
    "nett = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2))\n",
    "\n",
    "# Prior (a.k.a. the base distribution): Gaussian # init with independence as covariance = 0\n",
    "prior = torch.distributions.MultivariateNormal(torch.zeros(D), torch.eye(D))\n",
    "# Init RealNVP\n",
    "model = RealNVP(nets, nett, num_flows, prior, D=D, dequantization=True)\n",
    "# Print the summary (like in Keras)\n",
    "print(summary(model, torch.zeros(1, 64), show_input=False, show_hierarchical=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play! Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val nll=352.76339285714283\n",
      "saved!\n",
      "Epoch: 1, val nll=298.04169921875\n",
      "saved!\n",
      "Epoch: 2, val nll=275.17751255580356\n",
      "saved!\n",
      "Epoch: 3, val nll=260.0316727120536\n",
      "saved!\n",
      "Epoch: 4, val nll=248.23469587053572\n",
      "saved!\n",
      "Epoch: 5, val nll=238.5964341517857\n",
      "saved!\n",
      "Epoch: 6, val nll=230.52114815848213\n",
      "saved!\n",
      "Epoch: 7, val nll=224.1238295200893\n",
      "saved!\n",
      "Epoch: 8, val nll=218.80214006696428\n",
      "saved!\n",
      "Epoch: 9, val nll=214.40791155133928\n",
      "saved!\n",
      "Epoch: 10, val nll=210.71973074776787\n",
      "saved!\n",
      "Epoch: 11, val nll=207.74533761160714\n",
      "saved!\n",
      "Epoch: 12, val nll=204.95032505580357\n",
      "saved!\n",
      "Epoch: 13, val nll=202.55087890625\n",
      "saved!\n",
      "Epoch: 14, val nll=200.38552455357143\n",
      "saved!\n",
      "Epoch: 15, val nll=198.50694475446429\n",
      "saved!\n",
      "Epoch: 16, val nll=196.7733189174107\n",
      "saved!\n",
      "Epoch: 17, val nll=195.30576171875\n",
      "saved!\n",
      "Epoch: 18, val nll=193.84238839285715\n",
      "saved!\n",
      "Epoch: 19, val nll=192.57250558035713\n",
      "saved!\n",
      "Epoch: 20, val nll=191.33321707589286\n",
      "saved!\n",
      "Epoch: 21, val nll=190.3014174107143\n",
      "saved!\n",
      "Epoch: 22, val nll=189.20033342633928\n",
      "saved!\n",
      "Epoch: 23, val nll=188.01917131696428\n",
      "saved!\n",
      "Epoch: 24, val nll=187.07846121651787\n",
      "saved!\n",
      "Epoch: 25, val nll=186.046396484375\n",
      "saved!\n",
      "Epoch: 26, val nll=185.14707589285715\n",
      "saved!\n",
      "Epoch: 27, val nll=184.19130580357142\n",
      "saved!\n",
      "Epoch: 28, val nll=183.33776925223214\n",
      "saved!\n",
      "Epoch: 29, val nll=182.3576157924107\n",
      "saved!\n",
      "Epoch: 30, val nll=181.46161551339284\n",
      "saved!\n",
      "Epoch: 31, val nll=180.54182338169642\n",
      "saved!\n",
      "Epoch: 32, val nll=179.78283203125\n",
      "saved!\n",
      "Epoch: 33, val nll=178.90997349330357\n",
      "saved!\n",
      "Epoch: 34, val nll=178.06923130580358\n",
      "saved!\n",
      "Epoch: 35, val nll=177.11663643973213\n",
      "saved!\n",
      "Epoch: 36, val nll=176.43864815848215\n",
      "saved!\n",
      "Epoch: 37, val nll=175.70898716517857\n",
      "saved!\n",
      "Epoch: 38, val nll=174.64503766741072\n",
      "saved!\n",
      "Epoch: 39, val nll=174.05498744419643\n",
      "saved!\n",
      "Epoch: 40, val nll=173.31805803571427\n",
      "saved!\n",
      "Epoch: 41, val nll=172.2958203125\n",
      "saved!\n",
      "Epoch: 42, val nll=171.6778822544643\n",
      "saved!\n",
      "Epoch: 43, val nll=170.9613755580357\n",
      "saved!\n",
      "Epoch: 44, val nll=170.22749441964285\n",
      "saved!\n",
      "Epoch: 45, val nll=169.4614634486607\n",
      "saved!\n",
      "Epoch: 46, val nll=168.81745535714285\n",
      "saved!\n",
      "Epoch: 47, val nll=168.05589006696428\n",
      "saved!\n",
      "Epoch: 48, val nll=167.382783203125\n",
      "saved!\n",
      "Epoch: 49, val nll=166.7991322544643\n",
      "saved!\n",
      "Epoch: 50, val nll=165.80404017857143\n",
      "saved!\n",
      "Epoch: 51, val nll=165.55629743303572\n",
      "saved!\n",
      "Epoch: 52, val nll=164.75771065848215\n",
      "saved!\n",
      "Epoch: 53, val nll=164.15439174107144\n",
      "saved!\n",
      "Epoch: 54, val nll=163.37810267857142\n",
      "saved!\n",
      "Epoch: 55, val nll=162.92784319196429\n",
      "saved!\n",
      "Epoch: 56, val nll=162.37034877232142\n",
      "saved!\n",
      "Epoch: 57, val nll=161.6954603794643\n",
      "saved!\n",
      "Epoch: 58, val nll=161.317705078125\n",
      "saved!\n",
      "Epoch: 59, val nll=160.62921177455357\n",
      "saved!\n",
      "Epoch: 60, val nll=160.57399135044642\n",
      "saved!\n",
      "Epoch: 61, val nll=159.82363560267856\n",
      "saved!\n",
      "Epoch: 62, val nll=159.36836495535715\n",
      "saved!\n",
      "Epoch: 63, val nll=158.68986746651785\n",
      "saved!\n",
      "Epoch: 64, val nll=158.46373325892858\n",
      "saved!\n",
      "Epoch: 65, val nll=158.2531640625\n",
      "saved!\n",
      "Epoch: 66, val nll=157.12023158482143\n",
      "saved!\n",
      "Epoch: 67, val nll=156.76312779017857\n",
      "saved!\n",
      "Epoch: 68, val nll=156.68077427455358\n",
      "saved!\n",
      "Epoch: 69, val nll=156.51959681919644\n",
      "saved!\n",
      "Epoch: 70, val nll=155.30547712053573\n",
      "saved!\n",
      "Epoch: 71, val nll=155.51933872767856\n",
      "Epoch: 72, val nll=155.22749441964285\n",
      "saved!\n",
      "Epoch: 73, val nll=154.697119140625\n",
      "saved!\n",
      "Epoch: 74, val nll=154.4681821986607\n",
      "saved!\n",
      "Epoch: 75, val nll=154.15685546875\n",
      "saved!\n",
      "Epoch: 76, val nll=153.98730747767857\n",
      "saved!\n",
      "Epoch: 77, val nll=153.355908203125\n",
      "saved!\n",
      "Epoch: 78, val nll=153.6181263950893\n",
      "Epoch: 79, val nll=153.33811244419644\n",
      "saved!\n",
      "Epoch: 80, val nll=152.77852957589286\n",
      "saved!\n",
      "Epoch: 81, val nll=152.28968052455357\n",
      "saved!\n",
      "Epoch: 82, val nll=152.50729771205357\n",
      "Epoch: 83, val nll=151.40459402901786\n",
      "saved!\n",
      "Epoch: 84, val nll=151.06239397321428\n",
      "saved!\n",
      "Epoch: 85, val nll=151.7676967075893\n",
      "Epoch: 86, val nll=151.37172712053572\n",
      "Epoch: 87, val nll=150.27007672991073\n",
      "saved!\n",
      "Epoch: 88, val nll=150.80050502232143\n",
      "Epoch: 89, val nll=150.77817940848215\n",
      "Epoch: 90, val nll=150.28935128348215\n",
      "Epoch: 91, val nll=150.0081640625\n",
      "saved!\n",
      "Epoch: 92, val nll=150.11448939732142\n",
      "Epoch: 93, val nll=150.216982421875\n",
      "Epoch: 94, val nll=149.47599888392858\n",
      "saved!\n",
      "Epoch: 95, val nll=150.07215401785714\n",
      "Epoch: 96, val nll=149.82775809151786\n",
      "Epoch: 97, val nll=149.2690513392857\n",
      "saved!\n",
      "Epoch: 98, val nll=148.78301478794643\n",
      "saved!\n",
      "Epoch: 99, val nll=149.3873046875\n",
      "Epoch: 100, val nll=148.74169363839286\n",
      "saved!\n",
      "Epoch: 101, val nll=149.07751674107143\n",
      "Epoch: 102, val nll=148.88821986607144\n",
      "Epoch: 103, val nll=149.52366350446428\n",
      "Epoch: 104, val nll=148.66966657366072\n",
      "saved!\n",
      "Epoch: 105, val nll=148.61189732142856\n",
      "saved!\n",
      "Epoch: 106, val nll=148.5383984375\n",
      "saved!\n",
      "Epoch: 107, val nll=149.00718052455358\n",
      "Epoch: 108, val nll=149.36031529017856\n",
      "Epoch: 109, val nll=149.17385044642856\n",
      "Epoch: 110, val nll=148.12469308035713\n",
      "saved!\n",
      "Epoch: 111, val nll=148.37409598214285\n",
      "Epoch: 112, val nll=149.450927734375\n",
      "Epoch: 113, val nll=149.07209542410715\n",
      "Epoch: 114, val nll=147.74670340401786\n",
      "saved!\n",
      "Epoch: 115, val nll=149.11237862723215\n",
      "Epoch: 116, val nll=147.69863560267856\n",
      "saved!\n",
      "Epoch: 117, val nll=149.23076171875\n",
      "Epoch: 118, val nll=148.29148018973214\n",
      "Epoch: 119, val nll=148.54317661830356\n",
      "Epoch: 120, val nll=147.189443359375\n",
      "saved!\n",
      "Epoch: 121, val nll=148.48222237723215\n",
      "Epoch: 122, val nll=148.58287667410715\n",
      "Epoch: 123, val nll=148.00829799107143\n",
      "Epoch: 124, val nll=148.75451032366072\n",
      "Epoch: 125, val nll=149.62503208705357\n",
      "Epoch: 126, val nll=148.72889927455358\n",
      "Epoch: 127, val nll=148.88418526785713\n",
      "Epoch: 128, val nll=148.93387137276787\n",
      "Epoch: 129, val nll=149.20980747767857\n",
      "Epoch: 130, val nll=148.35142996651786\n",
      "Epoch: 131, val nll=149.8223325892857\n",
      "Epoch: 132, val nll=148.846982421875\n",
      "Epoch: 133, val nll=149.13669084821427\n",
      "Epoch: 134, val nll=149.80928571428572\n",
      "Epoch: 135, val nll=148.9106431361607\n",
      "Epoch: 136, val nll=149.847314453125\n",
      "Epoch: 137, val nll=150.65318498883929\n",
      "Epoch: 138, val nll=149.33427455357142\n",
      "Epoch: 139, val nll=149.23255998883928\n",
      "Epoch: 140, val nll=149.88508649553572\n",
      "Epoch: 141, val nll=150.40860770089284\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "nll_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
    "                       training_loader=training_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL LOSS: nll=138.08441257690157\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "\n",
    "samples_real(result_dir + name, test_loader)\n",
    "\n",
    "plot_curve(result_dir + name, nll_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Well, the nll value is also worse than the baseline. It seems like overfitting around epoch 100. The learning curve is \n",
    "smoother than the baseline. Maybe RealNVP is just a simple model, which means adding more complex operation will make it too\n",
    "complex to perform well.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
