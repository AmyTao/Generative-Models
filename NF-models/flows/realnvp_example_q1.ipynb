{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_model_summary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**\n",
    "\n",
    "The presented code is not optimized, it serves an educational purpose. It is written for CPU, it uses only fully-connected networks and an extremely simplistic dataset. However, it contains all components that can help to understand how RealNVP works, and it should be rather easy to extend it to more sophisticated models. This code could be run almost on any laptop/PC, and it takes a couple of minutes top to get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we go wild and use a dataset that is simpler than MNIST! We use a scipy dataset called Digits. It consists of ~1500 images of size 8x8, and each pixel can take values in $\\{0, 1, \\ldots, 16\\}$.\n",
    "\n",
    "The goal of using this dataset is that everyone can run it on a laptop, without any gpu etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "        elif mode == 'val':\n",
    "            self.data = digits.data[1000:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1350:].astype(np.float32)\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RealNVP code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the blogpost for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    \n",
    "    def __init__(self, nets, nett, num_flows, prior, D=2, dequantization=True):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        print('RealNVP by JT.')\n",
    "        \n",
    "        self.dequantization = dequantization\n",
    "        \n",
    "        self.prior = prior\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(num_flows)])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(num_flows)])\n",
    "        self.num_flows = num_flows\n",
    "        \n",
    "        self.D = D\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        # x: input, either images (for the first transformation) or outputs from the previous transformation\n",
    "        # index: it determines the index of the transformation\n",
    "        # forward: whether it is a pass from x to y (forward=True), or from y to x (forward=False)\n",
    "        \n",
    "        # TODO: Q1\n",
    "        (xa, xb) = torch.chunk(x, 2, 1) # split into 2 parts in dim=1\n",
    "\n",
    "\n",
    "        \n",
    "        s = self.s[index](xa)\n",
    "        t = self.t[index](xa)\n",
    "        \n",
    "        if forward:\n",
    "            #yb = f^{-1}(x)\n",
    "            yb = (xb - t) * torch.exp(-s)\n",
    "        else:\n",
    "            #xb = f(y)\n",
    "            yb = torch.exp(s) * xb + t\n",
    "        \n",
    "        return torch.cat((xa, yb), 1), s # recombine two parts along dim =1, return scale\n",
    "\n",
    "    def permute(self, x):\n",
    "        \n",
    "        return x.flip(1)\n",
    "\n",
    "\n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in range(self.num_flows):\n",
    "            z, s = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "            \n",
    "            log_det_J = log_det_J - s.sum(dim=1)\n",
    "            \n",
    "             \n",
    "\n",
    "        return z, log_det_J\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            \n",
    "            x, _ = self.coupling(x, i, forward=False)\n",
    "            \n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        z, log_det_J = self.f(x)\n",
    "        if reduction == 'sum':\n",
    "            return -(self.prior.log_prob(z) + log_det_J).sum()\n",
    "        else:\n",
    "            return -(self.prior.log_prob(z) + log_det_J).mean()\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        z = self.prior.sample((batchSize, self.D)) #每一个维度的值有相关性，考虑prior是多元分布\n",
    "        z = z[:, 0, :] # 对z进行操作，抛弃了其中一个维度\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(-1, self.D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions: training, evaluation, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather self-explanatory, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, test_batch in enumerate(test_loader):\n",
    "        if hasattr(model, 'dequantization'):\n",
    "            if model.dequantization:\n",
    "                test_batch = test_batch + (1. - torch.rand(test_batch.shape))/2.\n",
    "        loss_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def samples_real(name, test_loader):\n",
    "    # REAL-------\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = next(iter(test_loader)).detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def samples_generated(name, data_loader, extra_name=''):\n",
    "    x = next(iter(data_loader)).detach().numpy()\n",
    "\n",
    "    # GENERATIONS-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = model_best.sample(num_x * num_y)\n",
    "    x = x.detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_curve(name, nll_val):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('nll')\n",
    "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            if hasattr(model, 'dequantization'):\n",
    "                if model.dequantization:\n",
    "                    batch = batch + (1. - torch.rand(batch.shape))/2.\n",
    "            loss = model.forward(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "        else:\n",
    "            if loss_val < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_val\n",
    "                patience = 0\n",
    "\n",
    "                samples_generated(name, val_loader, extra_name=\"_epoch_\" + str(e))\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Digits(mode='train')\n",
    "val_data = Digits(mode='val')\n",
    "test_data = Digits(mode='test')\n",
    "\n",
    "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "result_dir = 'results_q1/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "name = 'realnvp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64   # input dimension\n",
    "M = 256  # the number of neurons in scale (s) and translation (t) nets\n",
    "\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 1000 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RealNVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealNVP by JT.\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1            [1, 256]           8,448           8,448\n",
      "       LeakyReLU-2            [1, 256]               0               0\n",
      "          Linear-3            [1, 256]          65,792          65,792\n",
      "       LeakyReLU-4            [1, 256]               0               0\n",
      "          Linear-5             [1, 32]           8,224           8,224\n",
      "            Tanh-6             [1, 32]               0               0\n",
      "          Linear-7            [1, 256]           8,448           8,448\n",
      "       LeakyReLU-8            [1, 256]               0               0\n",
      "          Linear-9            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-10            [1, 256]               0               0\n",
      "         Linear-11             [1, 32]           8,224           8,224\n",
      "         Linear-12            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-13            [1, 256]               0               0\n",
      "         Linear-14            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-15            [1, 256]               0               0\n",
      "         Linear-16             [1, 32]           8,224           8,224\n",
      "           Tanh-17             [1, 32]               0               0\n",
      "         Linear-18            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-19            [1, 256]               0               0\n",
      "         Linear-20            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-21            [1, 256]               0               0\n",
      "         Linear-22             [1, 32]           8,224           8,224\n",
      "         Linear-23            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-24            [1, 256]               0               0\n",
      "         Linear-25            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-26            [1, 256]               0               0\n",
      "         Linear-27             [1, 32]           8,224           8,224\n",
      "           Tanh-28             [1, 32]               0               0\n",
      "         Linear-29            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-30            [1, 256]               0               0\n",
      "         Linear-31            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-32            [1, 256]               0               0\n",
      "         Linear-33             [1, 32]           8,224           8,224\n",
      "         Linear-34            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-35            [1, 256]               0               0\n",
      "         Linear-36            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-37            [1, 256]               0               0\n",
      "         Linear-38             [1, 32]           8,224           8,224\n",
      "           Tanh-39             [1, 32]               0               0\n",
      "         Linear-40            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-41            [1, 256]               0               0\n",
      "         Linear-42            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-43            [1, 256]               0               0\n",
      "         Linear-44             [1, 32]           8,224           8,224\n",
      "         Linear-45            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-46            [1, 256]               0               0\n",
      "         Linear-47            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-48            [1, 256]               0               0\n",
      "         Linear-49             [1, 32]           8,224           8,224\n",
      "           Tanh-50             [1, 32]               0               0\n",
      "         Linear-51            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-52            [1, 256]               0               0\n",
      "         Linear-53            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-54            [1, 256]               0               0\n",
      "         Linear-55             [1, 32]           8,224           8,224\n",
      "         Linear-56            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-57            [1, 256]               0               0\n",
      "         Linear-58            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-59            [1, 256]               0               0\n",
      "         Linear-60             [1, 32]           8,224           8,224\n",
      "           Tanh-61             [1, 32]               0               0\n",
      "         Linear-62            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-63            [1, 256]               0               0\n",
      "         Linear-64            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-65            [1, 256]               0               0\n",
      "         Linear-66             [1, 32]           8,224           8,224\n",
      "         Linear-67            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-68            [1, 256]               0               0\n",
      "         Linear-69            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-70            [1, 256]               0               0\n",
      "         Linear-71             [1, 32]           8,224           8,224\n",
      "           Tanh-72             [1, 32]               0               0\n",
      "         Linear-73            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-74            [1, 256]               0               0\n",
      "         Linear-75            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-76            [1, 256]               0               0\n",
      "         Linear-77             [1, 32]           8,224           8,224\n",
      "         Linear-78            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-79            [1, 256]               0               0\n",
      "         Linear-80            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-81            [1, 256]               0               0\n",
      "         Linear-82             [1, 32]           8,224           8,224\n",
      "           Tanh-83             [1, 32]               0               0\n",
      "         Linear-84            [1, 256]           8,448           8,448\n",
      "      LeakyReLU-85            [1, 256]               0               0\n",
      "         Linear-86            [1, 256]          65,792          65,792\n",
      "      LeakyReLU-87            [1, 256]               0               0\n",
      "         Linear-88             [1, 32]           8,224           8,224\n",
      "=======================================================================\n",
      "Total params: 1,319,424\n",
      "Trainable params: 1,319,424\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The number of invertible transformations\n",
    "num_flows = 8\n",
    "\n",
    "# scale (s) network i.e.variance\n",
    "nets = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2), nn.Tanh())\n",
    "\n",
    "# translation (t) network i.e. mean\n",
    "nett = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2))\n",
    "\n",
    "# Prior (a.k.a. the base distribution): Gaussian # init with independence as covariance = 0\n",
    "prior = torch.distributions.MultivariateNormal(torch.zeros(D), torch.eye(D))\n",
    "# Init RealNVP\n",
    "model = RealNVP(nets, nett, num_flows, prior, D=D, dequantization=True)\n",
    "# Print the summary (like in Keras)\n",
    "print(summary(model, torch.zeros(1, 64), show_input=False, show_hierarchical=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play! Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val nll=226.3183984375\n",
      "saved!\n",
      "Epoch: 1, val nll=199.23556361607143\n",
      "saved!\n",
      "Epoch: 2, val nll=186.75920200892858\n",
      "saved!\n",
      "Epoch: 3, val nll=178.82648158482144\n",
      "saved!\n",
      "Epoch: 4, val nll=173.18832728794644\n",
      "saved!\n",
      "Epoch: 5, val nll=168.76352678571428\n",
      "saved!\n",
      "Epoch: 6, val nll=165.72785853794642\n",
      "saved!\n",
      "Epoch: 7, val nll=163.70657784598214\n",
      "saved!\n",
      "Epoch: 8, val nll=162.23069893973215\n",
      "saved!\n",
      "Epoch: 9, val nll=160.78246372767856\n",
      "saved!\n",
      "Epoch: 10, val nll=160.2451841517857\n",
      "saved!\n",
      "Epoch: 11, val nll=159.40016183035715\n",
      "saved!\n",
      "Epoch: 12, val nll=159.1037890625\n",
      "saved!\n",
      "Epoch: 13, val nll=157.38038643973215\n",
      "saved!\n",
      "Epoch: 14, val nll=157.13732840401786\n",
      "saved!\n",
      "Epoch: 15, val nll=155.88135044642857\n",
      "saved!\n",
      "Epoch: 16, val nll=155.43520368303572\n",
      "saved!\n",
      "Epoch: 17, val nll=155.15835518973213\n",
      "saved!\n",
      "Epoch: 18, val nll=153.39537806919643\n",
      "saved!\n",
      "Epoch: 19, val nll=153.42845284598215\n",
      "Epoch: 20, val nll=153.041259765625\n",
      "saved!\n",
      "Epoch: 21, val nll=152.76445731026786\n",
      "saved!\n",
      "Epoch: 22, val nll=152.62525530133928\n",
      "saved!\n",
      "Epoch: 23, val nll=151.97317940848214\n",
      "saved!\n",
      "Epoch: 24, val nll=150.42957589285714\n",
      "saved!\n",
      "Epoch: 25, val nll=150.64496512276784\n",
      "Epoch: 26, val nll=149.638505859375\n",
      "saved!\n",
      "Epoch: 27, val nll=150.28798270089285\n",
      "Epoch: 28, val nll=149.46702566964285\n",
      "saved!\n",
      "Epoch: 29, val nll=148.89172154017857\n",
      "saved!\n",
      "Epoch: 30, val nll=147.94433035714286\n",
      "saved!\n",
      "Epoch: 31, val nll=148.20144391741073\n",
      "Epoch: 32, val nll=146.448466796875\n",
      "saved!\n",
      "Epoch: 33, val nll=146.17223214285715\n",
      "saved!\n",
      "Epoch: 34, val nll=145.7655943080357\n",
      "saved!\n",
      "Epoch: 35, val nll=145.22363002232143\n",
      "saved!\n",
      "Epoch: 36, val nll=144.89933314732144\n",
      "saved!\n",
      "Epoch: 37, val nll=143.70971400669643\n",
      "saved!\n",
      "Epoch: 38, val nll=143.54242466517857\n",
      "saved!\n",
      "Epoch: 39, val nll=143.98245535714287\n",
      "Epoch: 40, val nll=142.97468470982142\n",
      "saved!\n",
      "Epoch: 41, val nll=143.73405691964285\n",
      "Epoch: 42, val nll=143.74507533482142\n",
      "Epoch: 43, val nll=141.80828683035713\n",
      "saved!\n",
      "Epoch: 44, val nll=142.29496233258928\n",
      "Epoch: 45, val nll=142.10477120535714\n",
      "Epoch: 46, val nll=142.2943540736607\n",
      "Epoch: 47, val nll=140.952568359375\n",
      "saved!\n",
      "Epoch: 48, val nll=140.88890904017856\n",
      "saved!\n",
      "Epoch: 49, val nll=141.1334779575893\n",
      "Epoch: 50, val nll=142.25260184151784\n",
      "Epoch: 51, val nll=140.73057059151785\n",
      "saved!\n",
      "Epoch: 52, val nll=140.42790318080358\n",
      "saved!\n",
      "Epoch: 53, val nll=142.19641880580357\n",
      "Epoch: 54, val nll=140.38580357142857\n",
      "saved!\n",
      "Epoch: 55, val nll=140.1075013950893\n",
      "saved!\n",
      "Epoch: 56, val nll=139.78587053571428\n",
      "saved!\n",
      "Epoch: 57, val nll=141.02944893973213\n",
      "Epoch: 58, val nll=137.15789760044643\n",
      "saved!\n",
      "Epoch: 59, val nll=140.4275795200893\n",
      "Epoch: 60, val nll=139.22203264508929\n",
      "Epoch: 61, val nll=135.77866908482142\n",
      "saved!\n",
      "Epoch: 62, val nll=135.83689592633928\n",
      "Epoch: 63, val nll=138.8501185825893\n",
      "Epoch: 64, val nll=134.91024274553573\n",
      "saved!\n",
      "Epoch: 65, val nll=134.09294642857142\n",
      "saved!\n",
      "Epoch: 66, val nll=135.98816545758928\n",
      "Epoch: 67, val nll=133.01418805803573\n",
      "saved!\n",
      "Epoch: 68, val nll=133.87180803571428\n",
      "Epoch: 69, val nll=132.62865792410713\n",
      "saved!\n",
      "Epoch: 70, val nll=133.14779854910714\n",
      "Epoch: 71, val nll=134.740478515625\n",
      "Epoch: 72, val nll=131.82466238839285\n",
      "saved!\n",
      "Epoch: 73, val nll=132.27354213169642\n",
      "Epoch: 74, val nll=132.78913643973215\n",
      "Epoch: 75, val nll=130.63438337053572\n",
      "saved!\n",
      "Epoch: 76, val nll=131.39093191964287\n",
      "Epoch: 77, val nll=132.42357282366072\n",
      "Epoch: 78, val nll=130.76600167410714\n",
      "Epoch: 79, val nll=130.10612025669644\n",
      "saved!\n",
      "Epoch: 80, val nll=130.58387137276785\n",
      "Epoch: 81, val nll=130.53450055803572\n",
      "Epoch: 82, val nll=128.82911551339285\n",
      "saved!\n",
      "Epoch: 83, val nll=129.78183733258928\n",
      "Epoch: 84, val nll=129.899052734375\n",
      "Epoch: 85, val nll=128.55648018973216\n",
      "saved!\n",
      "Epoch: 86, val nll=127.73914760044643\n",
      "saved!\n",
      "Epoch: 87, val nll=129.18838448660713\n",
      "Epoch: 88, val nll=128.02894810267858\n",
      "Epoch: 89, val nll=128.24881975446428\n",
      "Epoch: 90, val nll=127.17251395089286\n",
      "saved!\n",
      "Epoch: 91, val nll=126.03558872767857\n",
      "saved!\n",
      "Epoch: 92, val nll=126.91966238839285\n",
      "Epoch: 93, val nll=127.50292550223215\n",
      "Epoch: 94, val nll=126.23271484375\n",
      "Epoch: 95, val nll=127.22165597098214\n",
      "Epoch: 96, val nll=127.92421875\n",
      "Epoch: 97, val nll=126.42392857142858\n",
      "Epoch: 98, val nll=127.63964564732143\n",
      "Epoch: 99, val nll=126.35389369419643\n",
      "Epoch: 100, val nll=128.31516043526787\n",
      "Epoch: 101, val nll=126.25872907366072\n",
      "Epoch: 102, val nll=127.53108956473214\n",
      "Epoch: 103, val nll=128.225302734375\n",
      "Epoch: 104, val nll=126.05906947544644\n",
      "Epoch: 105, val nll=125.01094029017857\n",
      "saved!\n",
      "Epoch: 106, val nll=126.81145228794642\n",
      "Epoch: 107, val nll=125.89991350446428\n",
      "Epoch: 108, val nll=129.07869977678573\n",
      "Epoch: 109, val nll=127.27458844866071\n",
      "Epoch: 110, val nll=129.7073423549107\n",
      "Epoch: 111, val nll=127.38864536830357\n",
      "Epoch: 112, val nll=127.86839704241072\n",
      "Epoch: 113, val nll=127.92300223214286\n",
      "Epoch: 114, val nll=131.43764369419642\n",
      "Epoch: 115, val nll=129.7342564174107\n",
      "Epoch: 116, val nll=126.47208286830357\n",
      "Epoch: 117, val nll=129.4998060825893\n",
      "Epoch: 118, val nll=129.3925111607143\n",
      "Epoch: 119, val nll=125.79958147321429\n",
      "Epoch: 120, val nll=131.87540318080357\n",
      "Epoch: 121, val nll=131.45284040178572\n",
      "Epoch: 122, val nll=128.33353794642858\n",
      "Epoch: 123, val nll=129.67826171875\n",
      "Epoch: 124, val nll=129.25984933035716\n",
      "Epoch: 125, val nll=128.52306501116072\n",
      "Epoch: 126, val nll=128.259619140625\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "nll_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
    "                       training_loader=training_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL LOSS: nll=115.43002175964764\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "\n",
    "samples_real(result_dir + name, test_loader)\n",
    "\n",
    "plot_curve(result_dir + name, nll_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the fundamental RealNVP model. It use channel-wise mask and use simple Affine coupling layer. \n",
    "The output learning curve is a little bit wavy, but still converges around epoch 126. We can tell from the generated image\n",
    "that the model learnt some features of number. It is blurry because we only use 64 dimension. \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
